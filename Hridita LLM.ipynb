{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "access_token = '...'\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=access_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list_llm = [[0, 'The data shows a set of step counts aligned with specific sleep states and their associated durations, offering a more granular view of human behavior over a measured period. The first sequence presents a dense and highly variable distribution of step counts. These fluctuations could represent periods of high activity interspersed with rest, or they might reflect irregular daily routines influenced by external factors such as work schedules, exercise habits, or lifestyle choices. The second sequence focuses on sleep phases—wake, light, deep, and REM—suggesting the underlying complexity of sleep architecture. Transitioning between these stages does not follow a simple, repetitive pattern, indicating that various physiological and environmental triggers might influence how a person cycles through these states. The third sequence assigns durations to each sleep state, revealing how long an individual spends in each phase. These duration data points vary greatly, hinting at potential sleep disturbances, stress factors, or inconsistencies in sleep hygiene. From a modeling perspective, this kind of multivariate data requires methods that can handle temporal dynamics, non-linear relationships, and potentially missing or noisy data. Techniques like recurrent neural networks, hidden Markov models, or other sequence-aware algorithms can be valuable. Additionally, feature extraction and correlation analysis between step counts and subsequent sleep stage durations could illuminate how daytime activity patterns influence nighttime sleep quality and stability.'],\n",
    "#                  [0, 'The dataset again aligns three main data streams—step counts, sleep stages, and sleep durations—to reveal a complex interplay between daily activity and nighttime rest. The first sequence indicates substantial variability in step counts, reflecting highly irregular patterns of physical activity. Such fluctuations might be influenced by differing daily routines, stressors, work shifts, or inconsistent exercise habits. The second sequence, capturing sleep stages like wake, light, deep, REM, asleep, restless, and awake, shows that the person’s sleep architecture is equally dynamic. The presence of phases like ‘asleep’ and ‘restless,’ alongside standard sleep stages, suggests that this individual may experience significant disturbance or variability in sleep quality. The third sequence provides the duration of each sleep state, presenting an extensive range of values that highlight the non-uniformity of how long the person remains in each stage. This complexity suggests that simple, linear approaches may not be sufficient to model or predict sleep behavior. Advanced, sequence-aware methods—such as machine learning models capable of detecting non-linear patterns or hidden state transitions—could be employed to uncover underlying factors that shape these activity and sleep dynamics. By doing so, we may identify meaningful correlations and causal relationships that, in turn, could inform interventions or lifestyle adjustments aimed at improving sleep quality and overall well-being.'],\n",
    "#                  [0, 'the dataset expands in both complexity and length, further illustrating the intricate interplay of activity levels, sleep states, and temporal durations. The first sequence’s step counts remain highly erratic, with a wide range of values that suggest fragmented, unpredictable patterns of movement throughout the observed period. Compared to previous examples, the second sequence now includes an even richer set of sleep and wake states—such as wake, light, deep, REM, asleep, restless, and awake—plus additional transitions and repetitions. These states appear to shift frequently, reflecting a sleep architecture that is neither strictly cyclical nor easily predictable. The third sequence, representing corresponding durations, shows an extensive variety of time intervals spent in each state. These durations fluctuate markedly and do not settle into stable, recurring patterns over time. For modeling, this scenario reinforces the need for robust analytical techniques. Simple approaches may fail to capture the dynamic, non-linear, and context-dependent nature of these data. Instead, sophisticated sequence-aware models—like deep recurrent neural networks, attention-based transformers, or advanced state-space models—could prove more suitable. These methods can handle the complexity and variability present in the data, identify subtle correlations or causal links, and ultimately support more informed decision-making about interventions to improve sleep quality and overall well-being.'],\n",
    "#                  [1, \"the dataset is once again composed of three aligned sequences—step counts, sleep/wake states, and corresponding durations—that paint a comprehensive picture of an individual's daily activity and sleep patterns. The first sequence, a long and highly variable list of steps, suggests that the person’s movement may fluctuate drastically throughout the recorded period, hinting at an irregular or unpredictable lifestyle. The second sequence details a wide spectrum of sleep-related states including wake, light, deep, REM, asleep, restless, and awake, interspersed with transitions that appear neither uniform nor periodic. Instead, the individual’s sleep architecture seems layered with complexity, shifting through states at irregular intervals. The third sequence provides durations for these recorded sleep phases, further emphasizing the non-linear and dynamic nature of the observed behavior. The considerable variability in how long each state persists suggests that simple averages or basic statistical methods may not capture the underlying structure. This complex interplay of step counts, sleep states, and durations once again points toward the necessity of advanced, context-aware modeling methods. Techniques capable of handling large, irregular, and heterogeneous time-series data—such as deep learning models with attention mechanisms or hybrid statistical-machine learning approaches—may be best suited for extracting meaningful patterns, identifying causal factors, and ultimately guiding interventions aimed at improving sleep quality and activity balance.\"],\n",
    "#                  [1, \"This new dataset similarly presents three core sequences—steps, sleep/wake states, and durations—yet it is far more expansive and seemingly even more irregular than prior examples. The first sequence offers a staggering range of step counts, potentially spanning months or even years of intermittent recordings. This variability in steps suggests periods of intense activity contrasted with stretches of near-sedentary behavior. The second sequence describes a dense tapestry of sleep states, including wake, light, deep, REM, asleep, restless, and awake phases, interspersed with transitions that appear chaotic and non-repetitive. Notably, it also introduces more extended sequences of “restless” and “asleep” states that disrupt standard sleep architecture. The third sequence, representing durations, features a vast assortment of time intervals, from very short to extremely long, which makes discerning any stable cycle or rhythm challenging. From a modeling standpoint, this complexity and scale underscore the need for sophisticated, data-driven approaches. Traditional methods might fail to capture the subtle patterns hidden within the chaos. Instead, advanced machine learning models—especially those designed for high-dimensional, irregular time-series data—may be required to identify latent structures, correlate daytime activities with nocturnal rest quality, and ultimately provide actionable insights to improve sleep hygiene and overall health.\"],\n",
    "#                  [1, \" the complexity has escalated to an extreme level, combining an enormous variety of step counts, a complex array of sleep/wake states, and a wide range of associated durations. The step count data is massive, varied, and likely spans a very long timeframe, suggesting activity patterns that may be influenced by numerous external factors, seasonal changes, or shifting lifestyle patterns. The sleep data now incorporates numerous states—wake, light, deep, REM, asleep, restless, and even extended sequences of disturbed sleep—which appear to follow no simple, repetitive architecture. The durations also exhibit astounding variability, from very short intervals to extremely prolonged periods in certain states. Such data may be rife with missing values, anomalies, and non-linear relationships that defy basic statistical or time-series models. To extract value from this data for model development, cutting-edge techniques are required. Methods like deep neural networks with attention mechanisms, hierarchical hidden Markov models, or advanced state-space models may be necessary to capture the intricate temporal dependencies and hidden patterns. Approaches from anomaly detection, transfer learning, and domain adaptation might also prove invaluable. Moreover, careful feature engineering—combining domain knowledge of sleep biology, circadian rhythms, and human behavior—could help reduce the noise and highlight essential signals. Ultimately, these data underscore that modeling human activity and sleep patterns is a profoundly complex task, demanding sophisticated tools that can integrate context, learn from sparse and irregular observations, and provide insights robust to the intrinsic variability of human behavior.\"]]\n",
    "# df_llm = pd.DataFrame(data=data_list_llm, columns=['label', 'text'])\n",
    "# df_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "df_llm = pd.read_excel('/Users/wyd2hu/Documents/SA39/Hridita/Data/data_for_LLM_TILES_19.xlsx')\n",
    "df_llm.rename(columns={'PID': 'participant_id', 'Narration': 'narration', 'class': 'label'}, inplace=True)\n",
    "df_llm['narration'] = df_llm['narration'].astype(str)\n",
    "\n",
    "df_llm = shuffle(df_llm, random_state = 1234)\n",
    "\n",
    "df_llm_pos = df_llm[df_llm['label'] == 1].iloc[:500]\n",
    "df_llm_neg = df_llm[df_llm['label'] == 0].iloc[:500]\n",
    "\n",
    "df_llm = pd.concat([df_llm_pos, df_llm_neg])\n",
    "df_llm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (AutoTokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments)\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, balanced_accuracy_score\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from tqdm import tqdm\n",
    "from transformers import set_seed\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "set_seed(42)\n",
    "df = df_llm.copy()\n",
    "\n",
    "crt_num_train_epochs = 5\n",
    "crt_r = 4\n",
    "crt_learning_rate = 5e-4\n",
    "\n",
    "crt_file_name = 'epoch_'+ str(crt_num_train_epochs) +'__r_lora_' + str(crt_r) +'__learning_rate_'+str(crt_learning_rate) +'__shape_'+str(df.shape[0]) +'_llma_1b.xlsx'\n",
    "\n",
    "# 2. Define a Custom Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# 3. Initialize Tokenizer and Model\n",
    "model_name = \"gpt2\"  # Use 'distilgpt2' for a smaller model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "# GPT2ForSequenceClassification is available in transformers v4.28+\n",
    "base_model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    pad_token_id=tokenizer.eos_token_id  # GPT-2 doesn't have a pad token, so use eos_token_id\n",
    ")\n",
    "\n",
    "# 4. Identify Correct Target Modules for LoRA\n",
    "def get_lora_target_modules(model):\n",
    "    target_modules = []\n",
    "    for name, module in model.named_modules():\n",
    "        if ('attn.c_attn' in name):\n",
    "            target_modules.append(name.split('.')[-1])  # Extract 'c_attn'\n",
    "    return target_modules\n",
    "\n",
    "target_modules = get_lora_target_modules(base_model)\n",
    "print(f\"Target modules for LoRA: {target_modules}\")\n",
    "\n",
    "# Verify target modules\n",
    "if not target_modules:\n",
    "    raise ValueError(\"No target modules found for LoRA. Please verify the module names.\")\n",
    "\n",
    "# 5. Apply PEFT (LoRA) Configuration with Correct Target Modules\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r= crt_r,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=target_modules  # ['c_attn', 'c_attn', ...]\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "\n",
    "# 6. Prepare for LOSO-CV\n",
    "participant_ids = df['participant_id'].unique()\n",
    "\n",
    "# Initialize lists to store results\n",
    "actual_classes = []\n",
    "predicted_classes = []\n",
    "predicted_probabilities = []\n",
    "\n",
    "# Define evaluation metric\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Iterate over each participant for LOSO-CV\n",
    "for test_id in tqdm(participant_ids, desc=\"LOSO-CV\"):\n",
    "    # Split the data\n",
    "    train_df = df[df['participant_id'] != test_id]\n",
    "    test_df = df[df['participant_id'] == test_id]\n",
    "    list_actual_class = test_df['label'].tolist()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(\n",
    "        texts=train_df['narration'].tolist(),\n",
    "        labels=train_df['label'].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    test_dataset = TextDataset(\n",
    "        texts=test_df['narration'].tolist(),\n",
    "        labels=test_df['label'].tolist(),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs= crt_num_train_epochs,\n",
    "        per_device_train_batch_size=4,  # Reduced batch size for memory constraints\n",
    "        per_device_eval_batch_size=4,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate= crt_learning_rate,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        disable_tqdm=True,  # Disable to reduce output\n",
    "        fp16=True if torch.cuda.is_available() else False  # Enable mixed precision if possible\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks = [EarlyStoppingCallback(early_stopping_patience=3)])\n",
    "    \n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "    crt_predicted_classes = np.argmax(predictions, axis=1)    \n",
    "\n",
    "    actual_classes.extend(label_ids)\n",
    "    predicted_classes.extend(crt_predicted_classes)\n",
    "    probabilities = F.softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "    predicted_probabilities.extend(probabilities[:, 1])\n",
    "\n",
    "    print(f1_score(y_true=list_actual_class, y_pred=crt_predicted_classes, average='macro'))\n",
    "    print(balanced_accuracy_score(y_true=list_actual_class, y_pred=crt_predicted_classes))\n",
    "    print(recall_score(y_true=list_actual_class, y_pred=crt_predicted_classes, pos_label=1))\n",
    "    print(recall_score(y_true=list_actual_class, y_pred=crt_predicted_classes, pos_label=0))\n",
    "\n",
    "    # Optionally, convert lists to DataFrame for better visualization\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': actual_classes,\n",
    "        'Predicted': predicted_classes,\n",
    "        'Probabilities': predicted_probabilities})\n",
    "\n",
    "    results_df.to_excel('/Users/wyd2hu/Documents/SA39/Hridita/Findings/TILES_19_Final LLM/'+crt_file_name, index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Actual Classes:\", actual_classes)\n",
    "print(\"Predicted Classes:\", predicted_classes)\n",
    "print(\"Predicted Probabilities:\", predicted_probabilities)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "performance_list_df = []\n",
    "loc_llm_perform = '/Users/wyd2hu/Documents/SA39/Hridita/Findings/Final LLM/'\n",
    "for model_file in os.listdir(loc_llm_perform):\n",
    "    print(model_file)\n",
    "    df_llm_perform = pd.read_excel(os.path.join(loc_llm_perform, model_file))\n",
    "    \n",
    "    list_actual_class = df_llm_perform['Actual'].tolist()\n",
    "    list_predicted_class = df_llm_perform['Predicted'].tolist()\n",
    "\n",
    "    print(f1_score(y_true=list_actual_class, y_pred=list_predicted_class, average='macro'))\n",
    "    print(balanced_accuracy_score(y_true=list_actual_class, y_pred=list_predicted_class))\n",
    "    print(recall_score(y_true=list_actual_class, y_pred=list_predicted_class, pos_label=1))\n",
    "    print(recall_score(y_true=list_actual_class, y_pred=list_predicted_class, pos_label=0))\n",
    "\n",
    "    splitted_f_name = model_file.split('__')\n",
    "\n",
    "    if '1000' in splitted_f_name[3]:\n",
    "        pos_neg_class_instance = ' positive_class: 500, neg_class: 500'\n",
    "    elif '2500' in splitted_f_name[3]:\n",
    "        pos_neg_class_instance = ' positive_class: 500, neg_class: 2000'\n",
    "    else:\n",
    "        pos_neg_class_instance = ' positive_class: 1500, neg_class: 1500'\n",
    "\n",
    "    performance_list_df.append([splitted_f_name[0] +'  '+ splitted_f_name[1] +'  '+ splitted_f_name[2] + pos_neg_class_instance,\n",
    "                                f1_score(y_true=list_actual_class, y_pred=list_predicted_class, average='macro'),\n",
    "                                balanced_accuracy_score(y_true=list_actual_class, y_pred=list_predicted_class),\n",
    "                                recall_score(y_true=list_actual_class, y_pred=list_predicted_class, pos_label=1),\n",
    "                                recall_score(y_true=list_actual_class, y_pred=list_predicted_class, pos_label=0),\n",
    "                                precision_score(y_true=list_actual_class, y_pred=list_predicted_class, average='macro')])\n",
    "                            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance = pd.DataFrame(data=performance_list_df, columns=['Hyperparameters and pos_neg class', 'f1', 'bal_score', 'sensitivity', 'specificity', 'precision'])\n",
    "df_performance.to_latex('/Users/wyd2hu/Documents/SA39/Hridita/Findings/llm_performance_tiles_18.tex', index=False)\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probabilities)\n",
    "probabilities = F.softmax(torch.tensor(predictions), dim=1).numpy()\n",
    "print(label_ids)\n",
    "print(probabilities[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame({\n",
    "#         'Actual': actual_classes,\n",
    "#         'Predicted': predicted_classes,\n",
    "#         'Probabilities': predicted_probabilities})\n",
    "print(len(actual_classes))\n",
    "print(len(predicted_classes))\n",
    "print(len(predicted_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "results_df = pd.read_excel('/Users/wyd2hu/Documents/SA39/Hridita/Findings/Final LLM/epoch_5__r_lora_4__learning_rate_0.0005__shape_3000_llma_1b.xlsx')\n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.scatter(x=list(range(1, results_df['Probabilities'].shape[0] + 1)), y=results_df['Probabilities'].tolist())\n",
    "fig.show()\n",
    "\n",
    "px.histogram(results_df['Probabilities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall_score(y_true=list_actual_class, y_pred=logits, pos_label=1))\n",
    "print(recall_score(y_true=list_actual_class, y_pred=logits, pos_label=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_actual_class = test_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operation theater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas  as pd\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score, recall_score, precision_score\n",
    "\n",
    "loc_llm_perform = '/Users/wyd2hu/Documents/SA39/Hridita/Findings/TILES_19_Final LLM/'\n",
    "for model_file in os.listdir(loc_llm_perform):\n",
    "    df_llm_perform = pd.read_excel(os.path.join(loc_llm_perform, model_file))\n",
    "    list_actual_class = df_llm_perform['Actual'].tolist()\n",
    "    list_predicted_class = df_llm_perform['Predicted'].tolist()\n",
    "\n",
    "    print(f1_score(y_true=list_actual_class, y_pred=list_predicted_class, average='macro'))\n",
    "    print(balanced_accuracy_score(y_true=list_actual_class, y_pred=list_predicted_class))\n",
    "    print(recall_score(y_true=list_actual_class, y_pred=list_predicted_class, pos_label=1))\n",
    "    print(recall_score(y_true=list_actual_class, y_pred=list_predicted_class, pos_label=0))\n",
    "    print(precision_score(y_true=list_actual_class, y_pred=list_predicted_class, average='macro'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
